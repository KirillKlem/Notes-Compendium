# Paper Notes Compendium

[üá∑üá∫ RU-version](README_RU.md)

Curated, plain-language notes on influential papers, *mostly in machine learning*.  
**I make these notes first and foremost for myself** ‚Äî to better digest ideas, track my learning, and quickly recall key results and intuitions.  
But I hope they might also be helpful for others ‚Äî whether you‚Äôre learning from scratch, onboarding to a research/codebase, or just want a clear summary without rereading every paper.

‚ú® *I aim to keep one article review per day, gradually building a structured reference, this is something like my challenge.* ‚ú®

---

## üìÑ Papers

| # | Title | Author(s) | Year | English Note | Russian Note |
|---|-------|-----------|------|--------------|--------------|
| 1 | [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167) | Sergey Ioffe, Christian Szegedy | 2015 | [EN](notes/en/BatchNorm.md) | [RU](notes/ru/BatchNorm.md) |
| 2 | [An Image is Worth 16√ó16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) | Alexey Dosovitskiy *et al.* | 2021 | [EN](notes/en/ViT.md) | [RU](notes/ru/ViT.md) |
| 3 | [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030) | Ze Liu *et al.* | 2021 | [EN](notes/en/Swin.md) | [RU](notes/ru/Swin.md) |
| 4 | [Swin Transformer V2: Scaling Up Capacity and Resolution](https://arxiv.org/abs/2111.09883) | Ze Liu *et al.* | 2022 | [EN](notes/en/SwinV2.md) | [RU](notes/ru/SwinV2.md) |
| 5 | [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239) | Jonathan Ho, Ajay Jain, Pieter Abbeel | 2020 | [EN](notes/en/DDPM.md) | [RU](notes/ru/DDPM.md) |

---

‚≠êÔ∏è **If you find these notes useful, feel free to leave a star ‚Äî it helps others discover the project!** ‚≠êÔ∏è

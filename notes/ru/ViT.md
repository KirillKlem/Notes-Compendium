---
Date: 2025-05-13
Title: "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
Author: "Google Research, Brain Team\r (Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby)"
Year: "2021"
Link: https://arxiv.org/abs/2010.11929
tags:
  - vit
  - ml
  - cv
  - fundamental
---
## Abstract 
Первая успешная попытка применения Transformer'а к изображениям. Несмотря на доминацию CNN, существующих в тот момент, данная статья положила начало большому количеству исследований вокруг ViT, в первую очередь, благодаря доказательству того, что ViT очень хорошо масштабируется, намного лучше CNN в масштабах огромных данных

## Freeform Notes
##### Очевидные проблемы на изображениях для Transformer'ов
С момента создания Transformer'а и attention'а было немало попыток применить их к изображениям. Самый очевидный способ был применить self-attention по всем пикселям, но метод был обречён на провал из-за того, что кол-во операций было пропорционально $R^4$ (при изображении $R$x$R$  пикселей), что приводило к <mark style="background: #FF5582A6;">768 Гб памяти для матриц внимания</mark>. Были и попытки комбинировать CNN с механизмами самообучающегося внимания, либо заменяя отдельные компоненты CNN, но сохраняя общую структуру, но всё упиралось либо в недостаточную эффективность этих методов (особенно при масштабировании, что было ключевым свойством в NLP для трансформеров), либо в вычислительную сложность при масштабировании

Стандартная архитектура: ![[Pasted image 20250513201300.png]]

##### Представление данных перед обучением
Тк трансформер работает с 1D последовательностью, а изображение является 2D, авторы статьи представляют изображение в $x_p \in \mathbb{R}^{H\times W \times C} \Rightarrow x_p \in \mathbb{R}^{N \times (P^2\cdot C)}$ , где $(H, W)$ - разрешение изображения, $C$ - каналы, $(P, P)$ - разрешение каждого патча, $N = HW/P^2$ - количество патчей 

Добавляется *[class]-токен* (идея из берта) в самое начало последовательности, то есть перед всеми патчами изображения - <mark style="background: #BBFABBA6;">extra learnable embedding</mark>. *Состояние этого токена на выходе Трансформер-энкодера и является представлением всего изображения.* 

Вместо фиксированного кодирования вектора, как в ванильной архитектуре Трансформера, здесь обучается <mark style="background: #ADCCFFA6;">position embeddings</mark> для того, чтобы добавить модели информацию о положении патча. Причём это оказалось лучше любых 2D фиксированных внедрений, поэтому оставили этот вариант. z0 = [xclass; x1pE; x2pE; · · · ; xN p E] + Epos, где Epos имеет размер (N+1) x D (количество патчей N плюс токен класса, умноженное на размерность D модели). Позиционные внедрения (Epos) представляют собой обучаемую матрицу размерности (N+1) x D, где N+1 соответствует количеству элементов в одномерной последовательности (токен класса + N патчей). К каждому элементу этой последовательности (токену класса и каждому внедрению патча) добавляется соответствующий вектор из матрицы Epos на основе его индекса (позиции) в этой одномерной последовательности

◦К токену класса (позиция 0) добавляется Epos[0, :]4.

◦К первому патчу в последовательности (позиция 1) добавляется Epos[1, :]4.

◦Ко второму патчу в последовательности (позиция 2) добавляется Epos[2, :]4.

◦...и так далее, до последнего патча (позиция N), к которому добавляется Epos[N, :].

Таким образом, связь между позиционным внедрением и 2D положением патча устанавливается не через вычисление внедрения из координат, а через присвоение определенного, уникального обучаемого вектора из матрицы Epos каждому патчу на основе его фиксированной позиции в одномерной последовательности4. Поскольку эта одномерная последовательность формируется из 2D патчей в систематическом порядке, индекс в последовательности (например, индекс k) всегда соответствует патчу, взятому из определенного 2D местоположения на исходном изображении..

*Изначально эти векторы в Epos могут быть случайными и "не несут информации о 2D положениях патчей"*. Однако, в процессе обучения модели на большом наборе данных, Трансформер учится использовать эти позиционные внедрения для понимания пространственных отношений между патчами

Каждый патч вытягивается в одномерный вектор (включая все 3 канала RGB). Если патч 16х16, то итоговый размер вектора 16 * 16 * 3 = 768, далее идёт обычный линейный слой.
В сумме с Epos и [class]-токен это и есть линейная проекция в архитектуре.

##### Transformer encoder
Весь энкодер состоит из L повторяющихся трансформер-блоков:
![[Pasted image 20250513212202.png]]
![[Pasted image 20250513212211.png]]
Под Norm подразумевается LayerNorm

##### Classification token и head
Classification token из начала является <mark style="background: #FFF3A3A6;">"концентратором информации"</mark> по изображению. Тк он сам не является никаким патчем и не несёт собственной информации, его основное содержание идёт из других патчей, тем самым он агрегирует её

Далее именно этот токен идёт в <mark style="background: #D2B3FFA6;">Classification Head</mark>, который представляет собой маленькую нейронную сеть, которая и выдаёт предсказания. В статье предлагаю на претрейне делать её обычным MLP с 1/2 слоями, а на файн-тюнинге просто линейный слой, т.е.
- *Pre-training*: `Classification Head(y) = Softmax(Linear2(ReLU(Linear1(y))))`
- *Fine-tuning*: `Classification Head(y) = Softmax(Linear(y))`


##### Inductive bias 
Inductive bias (индуктивное смещение) — это набор априорных предположений, которые модель машинного обучения делает относительно природы данных, которые она будет видеть во время обучения и тестирования. Эти предположения позволяют модели обобщать на новые, ранее невиданные данные.

**Тут выражается главное и, наверное, ключевое различие между ViT и CNN**:
1. <mark style="background: #CACFD9A6;">CNN</mark> имеет сильное индуктивное смещение, тк: 
	- <mark style="background: #FF5582A6;">Locality (Локальность)</mark>: CNNs обрабатывают изображения небольшими локальными участками (например, 3x3 пикселя). Это означает, что модель предполагает, что важная информация содержится в локальных окрестностях
		- *Повышает индуктивное смещение*: Ограничивая область "зрения" каждого слоя небольшими локальными окрестностями, мы предполагаем, что важные признаки находятся рядом друг с другом.
	    - *Понижает гибкость*: Модель может испытывать трудности с захватом долгосрочных зависимостей или глобального контекста.
	- <mark style="background: #FF5582A6;">Two-dimensional neighborhood structure (Двумерная структура окрестностей)</mark>: Сверточные слои учитывают пространственное расположение пикселей в окрестности. 
		- *Повышает индуктивное смещение*: Предполагается, что пространственное расположение пикселей важно и должно учитываться при обработке.
	    - *Ограничивает способы обработки данных*: Модель оптимизирована для обработки данных, организованных в двумерные сетки (изображения).
	- <mark style="background: #FF5582A6;">Translation equivariance (Инвариантность к сдвигу)</mark>: CNNs распознают объекты, даже если они сдвинуты в пределах изображения. Это достигается благодаря тому, что сверточные фильтры применяются к каждому участку изображения одинаково. Если объект сдвинуть, активация фильтра также сдвинется, но останется той же.
	    - *Повышает индуктивное смещение*: Предполагается, что местоположение объекта не должно влиять на его распознавание.
	    - *Ограничивает способность к обучению пространственно-зависимым признакам:* Модель менее чувствительна к конкретным местоположениям объектов, что может быть важно в некоторых задачах.
2. <mark style="background: #CACFD9A6;">ViT</mark> имеет меньшее индуктивное смещение, тк
	 - <mark style="background: #BBFABBA6;">Разбиение на патчи</mark>: ViT предполагает, что изображение можно разбить на патчи, но это очень слабое предположение.
		 - *Слабо повышает индуктивное смещение*: Предполагается, что изображение можно представить как набор патчей, что само по себе является довольно общим предположением.
	    - *Оставляет много свободы*: Модель может выучить взаимосвязи между патчами любым способом.
	- <mark style="background: #BBFABBA6;">Positional Embeddings</mark>: ViT использует positional embeddings, чтобы учитывать положение патчей, но эти embeddings изначально не содержат информации о 2D структуре изображения. Они просто указывают, какой патч идет каким по счету. Модель должна выучить взаимосвязи между положениями патчей с нуля.
		- *Немного повышает индуктивное смещение*: Предоставляет модели информацию о порядке патчей, но не о их 2D-положении в исходном изображении.
	    - *Требует обучения пространственным связям*: Модель должна выучить, как positional embeddings соотносятся с пространственными позициями патчей в изображении.
	- <mark style="background: #BBFABBA6;">Self-Attention</mark>: Self-attention слои глобальны, то есть каждый патч может взаимодействовать с любым другим патчем в изображении, независимо от расстояния между ними. Это дает модели большую гибкость, но также требует больше данных для обучения.
		- *Понижает индуктивное смещение*: Позволяет каждому патчу взаимодействовать с любым другим патчем, не делая никаких предположений о локальности или пространственной близости.
		- *Требует больше данных для обучения*: Модель должна выучить, какие взаимодействия важны, а какие нет, что требует большого количества данных.
	- <mark style="background: #BBFABBA6;">MLP слои</mark>: Только MLP слои в ViT являются локальными и инвариантными к сдвигу, но их влияние меньше, чем в CNN.
		-  *Немного повышают индуктивное смещение*: MLP слои в каждом патче работают локально и инвариантны к сдвигу, подобно сверточным слоям.
	    - *Ограниченное влияние*: Вклад MLP слоев в общее индуктивное смещение ViT меньше, чем у сверточных слоев в CNN, так как основную работу выполняет Self-Attention.

##### Гибридная архитектура
Можно заменять Linear projection в архитектуре на CNN feature maps с уменьшение каналов до 1 в конце

##### Эксперименты и зависимость от датасета
Обучение стандартное: Adam, lr-warmup, batchsize=512
Версии модели:

| Модель    | Слои | Размерность скрытого пространства | размерность MLP | Головы | Параметры |
| --------- | ---- | --------------------------------- | --------------- | ------ | --------- |
| ViT-Base  | 12   | 768                               | 3072            | 12     | 86М       |
| ViT-Large | 24   | 1024                              | 4096            | 16     | 307М      |
| ViT-Huge  | 32   | 1280                              | 5120            | 16     | 632М      |

Ключевое - датасет, модели приходится учить любые пространственные связи с нуля (в том числе из-за меньшего индуктивного байеса), тк в отличие от CNN он не уверен стоит ли смотреть на соседние пиксели или на совсем дальние, то есть вообще ничего не знает, поэтому он учит даже базовые вещи и зависит от количества данных, классический ImageNet имеет 1.3М изображений, расширенный 14М изображений, а JFT (датасет для обучения ViT) - 303М изображения в качестве выше ImageNet
![[Pasted image 20250513221157.png]]
![[Pasted image 20250513221230.png]]

Что поразительно и важно: ViT в разы легче параллелится, чем CNN из-за своей структуры и требует меньше компьюта для больших данных
![[Pasted image 20250513221340.png]]

![[Pasted image 20250513221418.png]]

После экспериментов выяснилось, что ViT идеально учит позицию патчей, а также, что несмотря на то, что ViT на ранних слоях также как и CNN смотрит в основном на ближайшие позиции и потом берёт всё более широкий контекст, он может смотреть и на разные части изображения уже в начале, если это требуется, что даёт им огромное преимущество относительно CNN
![[Pasted image 20250513221650.png]]


## Important Points

-  **Первые чистые Transformer'ы для CV**
	- ViT — первая модель, полностью отказавшаяся от свёрток в компьютерном зрении.
	- Она показала, что трансформеры превосходят ResNet, если обучать их на огромных датасетах.
	- Начало эпохи "attention везде" в CV.
-  **Патчи вместо пикселей + токенизация**
	- Изображение разбивается на фиксированные патчи (например, 16×16 пикселей).
	- Каждый патч разворачивается в вектор признаков и подаётся через линейный слой → получается токен.
	- Это единственный этап, где учитывается пространственная структура.
- **[CLS]-токен для классификации + минималистичная голова**
	- В начало последовательности добавляется специальный [CLS]-токен, который собирает глобальную информацию через self-attention.
	- На этапе дообучения используется маленькая 2-слойная MLP-голова; при финетюнинге — обычный линейный слой + softmax.
-  **Обучаемые позиционные эмбеддинги вместо фиксированных**
	- Позиционные эмбеддинги инициализируются случайно и обучаются вместе с моделью.
	- ViT сам учится понимать структуру изображения, без жёстко заданной сетки.
- **Очень слабые априорные предположения ⇒ нужны огромные данные**
	- В отличие от CNN, у ViT почти нет встроенных ограничений (нет локальности, нет сдвиговой эквариантности).
	- Поэтому модель очень прожорлива по данным — для нормального обучения требуется масштаб типа JFT-300M (303 млн изображений).
-  **Отличная масштабируемость и глобальное внимание**
	- Self-attention позволяет соединять любые части изображения уже на первом слое, если это нужно.
	- При этом ViT проще распараллелить по устройствам, чем глубокие CNN.
	- Модели типа ViT-Huge превосходят ResNet-152 и при этом требуют меньше FLOPs на изображение.